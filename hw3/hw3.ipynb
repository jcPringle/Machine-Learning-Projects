{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRTPf1K6evk2",
        "outputId": "b1ac4ef6-bd93-4dee-8212-8da130effd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis\n",
        "\n",
        "The central problem that I will be solving for this homework will be the classificiation of varius handwritten number raning zero to 9."
      ],
      "metadata": {
        "id": "N7ElAHridWhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading the dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/handwritten_numbers_dataset.csv')\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "uvAV1AiXexBr",
        "outputId": "a159ff7e-5615-4c28-c836-73afd3a367a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
              "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
              "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
              "\n",
              "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
              "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
              "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
              "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
              "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
              "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
              "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
              "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
              "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
              "\n",
              "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
              "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
              "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
              "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
              "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
              "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
              "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
              "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
              "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
              "\n",
              "       pixel781  pixel782  pixel783  \n",
              "count   42000.0   42000.0   42000.0  \n",
              "mean        0.0       0.0       0.0  \n",
              "std         0.0       0.0       0.0  \n",
              "min         0.0       0.0       0.0  \n",
              "25%         0.0       0.0       0.0  \n",
              "50%         0.0       0.0       0.0  \n",
              "75%         0.0       0.0       0.0  \n",
              "max         0.0       0.0       0.0  \n",
              "\n",
              "[8 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ed6b04a-e494-4bf4-a297-2a2282c30ce5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.00000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.000000</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "      <td>42000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.456643</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.219286</td>\n",
              "      <td>0.117095</td>\n",
              "      <td>0.059024</td>\n",
              "      <td>0.02019</td>\n",
              "      <td>0.017238</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.887730</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6.312890</td>\n",
              "      <td>4.633819</td>\n",
              "      <td>3.274488</td>\n",
              "      <td>1.75987</td>\n",
              "      <td>1.894498</td>\n",
              "      <td>0.414264</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>253.00000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 785 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ed6b04a-e494-4bf4-a297-2a2282c30ce5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ed6b04a-e494-4bf4-a297-2a2282c30ce5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ed6b04a-e494-4bf4-a297-2a2282c30ce5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-be07a4de-e999-4d19-9d54-c16030aa0521\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be07a4de-e999-4d19-9d54-c16030aa0521')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-be07a4de-e999-4d19-9d54-c16030aa0521 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#determine the number of instances of each label\n",
        "\n",
        "# determine the distribution of positive to negative classifications\n",
        "for l in range(10):\n",
        "  n = df['label'].value_counts()[l]\n",
        "  print(\"Label {}:, {:.2f}%\".format(l, n/df.shape[0] * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpS7zOTCdnE3",
        "outputId": "65044a0c-a70f-454c-db7b-abcc3a2914b5"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0:, 9.84%\n",
            "Label 1:, 11.15%\n",
            "Label 2:, 9.95%\n",
            "Label 3:, 10.36%\n",
            "Label 4:, 9.70%\n",
            "Label 5:, 9.04%\n",
            "Label 6:, 9.85%\n",
            "Label 7:, 10.48%\n",
            "Label 8:, 9.67%\n",
            "Label 9:, 9.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the differences between the proportion of the dataset that each label makes up is not very much, being at most 2 percent, I see no need to try and create a more even distribution for training purposes. In fact, to ignore some data for training could be detrimental to the models ability."
      ],
      "metadata": {
        "id": "KoNma7RqfFUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data\n",
        "\n",
        "ratio = 0.8 #the proportion of the full dataset dedicated\n",
        "rows = len(df)  #total number of rows in the full dataset\n",
        "\n",
        "split = int(rows * ratio)\n",
        "\n",
        "# Split data into testing and training data\n",
        "df_train = df[0:split]\n",
        "df_test = df[split:]"
      ],
      "metadata": {
        "id": "1DmUes1VgxoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#generates a tensor of dimension nin initialized with Glorot Scheme\n",
        "def glorot(nin, nout):\n",
        "    r = math.sqrt(6/(nin + nout))\n",
        "    return np.random.uniform(-r, r, (nin, nout))\n",
        "\n",
        "#ReLu Function\n",
        "def relu(x):\n",
        "    return max(0.0, x)\n",
        "\n",
        "#sigmoid function\n",
        "def sigmoid(x):\n",
        "  try:\n",
        "    #print(f\"sigmoid x: {x}\", flush = True)\n",
        "    return 1/(1 + math.exp(-x))\n",
        "  except OverflowError:\n",
        "    if(x > 0):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "#derivative of the ReLu function\n",
        "def dRelu(x):\n",
        "    if(x > 1):\n",
        "        return 1\n",
        "    else: #the derivative is actually 0 at x=0, but just assume it is 0\n",
        "        return 0\n",
        "\n",
        "#derivative of the sigmoid function\n",
        "def dSigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "class NeurelNetwork:\n",
        "\n",
        "    #n0 : input layer dimension\n",
        "    #n1, n2: hidden layer dimensions\n",
        "    #n3: output layer dimension\n",
        "    def __init__(self, n0, n1, n2, n3):\n",
        "      #initalize the weight\n",
        "      self.W1 = glorot(n1, n0) # hidden layer\n",
        "      self.W2 = glorot(n2, n1) # hidden layer\n",
        "      self.W3 = glorot(n3, n2) # output layer\n",
        "\n",
        "      #intialize the biases\n",
        "      self.b1 = np.zeros(n1, dtype=np.float32) # hidden laye 1\n",
        "      self.b2 = np.zeros(n2, dtype=np.float32) # hidden layer 2\n",
        "      self.b3 = np.zeros(n3, dtype=np.float32) # output layer\n",
        "\n",
        "      #initalize variables for Adam Optimizer\n",
        "      self.timeStep = 1\n",
        "\n",
        "      self.nuP_W1 = np.zeros((n1, n0))\n",
        "      self.hP_W1 = np.zeros((n1, n0))\n",
        "\n",
        "      self.nuP_W2 = np.zeros((n2, n1))\n",
        "      self.hP_W2 = np.zeros((n2, n1))\n",
        "\n",
        "      self.nuP_W3 = np.zeros((n3, n2))\n",
        "      self.hP_W3 = np.zeros((n3, n2))\n",
        "\n",
        "      self.nuP_b1 = np.zeros(n1)\n",
        "      self.hP_b1 = np.zeros(n1)\n",
        "\n",
        "      self.nuP_b2 = np.zeros(n2)\n",
        "      self.hP_b2 = np.zeros(n2)\n",
        "\n",
        "      self.nuP_b3 = np.zeros(n3)\n",
        "      self.hP_b3 = np.zeros(n3)\n",
        "\n",
        "    def h1(self, a):\n",
        "      return np.vectorize(relu)(a) #vectorize the ReLu function and apply it to the matrix\n",
        "\n",
        "    def h2(self, a):\n",
        "      return np.vectorize(relu)(a) #vectorize the ReLu function and apply it to the matrix\n",
        "\n",
        "    def h3(self, a):\n",
        "      return np.vectorize(sigmoid)(a) #vectorize the ReLu function and apply it to the matrix\n",
        "\n",
        "    def dh1(self, a):\n",
        "      return np.vectorize(dRelu)(a)\n",
        "\n",
        "    def dh2(self, a):\n",
        "      return np.vectorize(dRelu)(a)\n",
        "\n",
        "    def dh3(self, a):\n",
        "      return np.vectorize(dSigmoid)(a)\n",
        "\n",
        "    def loss(self, Xbatch, Ybatch):\n",
        "      #compute the output of the network using Xbatch\n",
        "\n",
        "      #print(f\"MATRIX: W1: {self.W1.shape}, XBATCH: {Xbatch.shape}, B1: {self.b1.shape}\", flush = True)\n",
        "\n",
        "      forward_a1 = np.matmul(self.W1, self.Xbatch) + self.b1[:, np.newaxis]\n",
        "      forward_o1 = self.h1(forward_a1) #apply the ReLu function\n",
        "\n",
        "      #print(f\"MATRIX: W2: {self.W2.shape}, o1: {self.o1.shape}, B2: {self.b2.shape}\", flush = True)\n",
        "\n",
        "      forward_a2 = np.matmul(self.W2,  forward_o1) + self.b2[:, np.newaxis]\n",
        "      forward_o2 = self.h2(forward_a2) #apply the ReLu function\n",
        "\n",
        "      #print(f\"MATRIX: W3: {self.W3.shape}, o2: {self.o2.shape}, B1: {self.b3.shape}\", flush = True)\n",
        "\n",
        "      forward_a3 = np.matmul(self.W3,  forward_o2) + self.b3[:, np.newaxis]\n",
        "      Ypreds= self.h2(forward_a3) #apply the Sigmoid function\n",
        "\n",
        "      #Retrieve the dimensions of the Xbatch\n",
        "      d, m = Xbatch.shape\n",
        "\n",
        "      #compute the loss using the Xbatch, and Ybatch\n",
        "      f = lambda x, y : -(x * np.log(y) + (1 - x) * np.log(1 - y)) #negative log likely-hood\n",
        "\n",
        "      #print(f\"YBATCH: {Ybatch.shape} YPREDS: {Ypreds.shape}\")\n",
        "\n",
        "\n",
        "      return np.mean(np.vectorize(f)(Ybatch, Ypreds))\n",
        "\n",
        "    def dloss(self, Ypreds):\n",
        "        #compute the derivative of the loss function with respect to Y (x-y/ y-y^2)\n",
        "        df = lambda x, y : (x-y)/max((y-y**2), 10e-8)\n",
        "\n",
        "        return np.vectorize(df)(self.Ybatch, Ypreds)\n",
        "\n",
        "    def forward(self, Xbatch):\n",
        "      #store the Xbatch as an instance\n",
        "      self.Xbatch = np.copy(Xbatch)\n",
        "\n",
        "      #print(f\"MATRIX: W1: {self.W1.shape}, XBATCH: {Xbatch.shape}, B1: {self.b1.shape}\", flush = True)\n",
        "\n",
        "      #store the activations and outputs as instance variables\n",
        "      self.a1 = np.matmul(self.W1, self.Xbatch) + self.b1[:, np.newaxis]\n",
        "      self.o1 = self.h1(self.a1) #apply the ReLu function\n",
        "\n",
        "      #print(f\"MATRIX: W2: {self.W2.shape}, o1: {self.o1.shape}, B2: {self.b2.shape}\", flush = True)\n",
        "\n",
        "      self.a2 = np.matmul(self.W2,  self.o1) + self.b2[:, np.newaxis]\n",
        "      self.o2 = self.h2(self.a2) #apply the ReLu function\n",
        "\n",
        "      #print(f\"MATRIX: W3: {self.W3.shape}, o2: {self.o2.shape}, B1: {self.b3.shape}\", flush = True)\n",
        "\n",
        "      self.a3 = np.matmul(self.W3,  self.o2) + self.b3[:, np.newaxis]\n",
        "      self.o3 = self.h3(self.a3) #apply the sigmoid function\n",
        "\n",
        "      return np.copy(self.o3)\n",
        "\n",
        "    def backward(self, Ybatch):\n",
        "      self.Ybatch = np.copy(Ybatch)\n",
        "\n",
        "      #store the gradient of the weights as an instance variables\n",
        "      J3 = self.dloss(self.o3) * self.dh3(self.a3)\n",
        "\n",
        "      self.dW3 = -np.dot(J3, self.o2.T)\n",
        "      self.db3 = -np.sum(J3, axis=1)\n",
        "\n",
        "      J2 = np.dot(self.W3.T, J3) * self.dh2(self.a2)\n",
        "\n",
        "      self.dW2 = -np.dot(J2, self.o1.T)\n",
        "      self.db2 = -np.sum(J2, axis=1)\n",
        "\n",
        "      J1 = np.dot(self.W2.T, J2) * self.dh1(self.a1)\n",
        "\n",
        "      self.dW1 = -np.dot(J1, self.Xbatch.T)\n",
        "      self.db1 = -np.sum(J1, axis=1)\n",
        "\n",
        "    def adam_step(self, alpha=0.0001, rho1=0.0, rho2=0.999, delta=1e-8):\n",
        "      t = self.timeStep\n",
        "\n",
        "      #compute for W3\n",
        "      self.nuP_W3 = rho1 * self.nuP_W3 + (1 - rho1) * self.dW3\n",
        "      self.hP_W3 = rho2 * self.hP_W3 + ((1 - rho2) * self.dW3) * self.dW3\n",
        "\n",
        "      nuPhat_W3 = self.nuP_W3/(1 - rho1**t)\n",
        "      hPhat_W3 = self.hP_W3/(1-rho2**t)\n",
        "\n",
        "      self.W3 = self.W3 - (alpha * nuPhat_W3)/(np.sqrt(hPhat_W3) + delta)\n",
        "\n",
        "      #compute for b3\n",
        "      self.nuP_b3 = rho1 * self.nuP_b3 + (1 - rho1) * self.db3\n",
        "      self.hP_b3 = rho2 * self.hP_b3 + ((1 - rho2) * self.db3) * self.db3\n",
        "\n",
        "      nuPhat_b3 = self.nuP_b3/(1 - rho1**t)\n",
        "      hPhat_b3 = self.hP_b3/(1-rho2**t)\n",
        "\n",
        "      self.b3 = self.b3 - (alpha * nuPhat_b3)/(np.sqrt(hPhat_b3) + delta)\n",
        "\n",
        "      #compute for W2\n",
        "      self.nuP_W2 = rho1 * self.nuP_W2 + (1 - rho1) * self.dW2\n",
        "      self.hP_W2 = rho2 * self.hP_W2 + ((1 - rho2) * self.dW2) * self.dW2\n",
        "\n",
        "      nuPhat_W2 = self.nuP_W2/(1 - rho1**t)\n",
        "      hPhat_W2 = self.hP_W2/(1-rho2**t)\n",
        "\n",
        "      self.W2 = self.W2 - (alpha * nuPhat_W2)/(np.sqrt(hPhat_W2) + delta)\n",
        "\n",
        "      #print(\"nuP_W2: {}\\nhP_W2: {}\\nnuPhat_W2: {}\\nhPhat_W2: {}\\n\\n\".format(self.nuP_W2.shape, self.nhP_W2.shape, ))\n",
        "\n",
        "      #compute for b2\n",
        "      self.nuP_b2 = rho1 * self.nuP_b2 + (1 - rho1) * self.db2\n",
        "      self.hP_b2 = rho2 * self.hP_b2 + ((1 - rho2) * self.db2) * self.db2\n",
        "\n",
        "      nuPhat_b2 = self.nuP_b2/(1 - rho1**t)\n",
        "      hPhat_b2 = self.hP_b2/(1-rho2**t)\n",
        "\n",
        "      self.b2 = self.b2 - (alpha * nuPhat_b2)/(np.sqrt(hPhat_b2) + delta)\n",
        "\n",
        "      #compute for W1\n",
        "      self.nuP_W1 = rho1 * self.nuP_W1 + (1 - rho1) * self.dW1\n",
        "      self.hP_W1 = rho2 * self.hP_W1 + ((1 - rho2) * self.dW1) * self.dW1\n",
        "\n",
        "      nuPhat_W1 = self.nuP_W1/(1 - rho1**t)\n",
        "      hPhat_W1 = self.hP_W1/(1-rho2**t)\n",
        "\n",
        "      self.W1 = self.W1 - (alpha * nuPhat_W1)/(np.sqrt(hPhat_W1) + delta)\n",
        "\n",
        "      #compute for b1\n",
        "      self.nuP_b1 = rho1 * self.nuP_b1 + (1 - rho1) * self.db1\n",
        "      self.hP_b1 = rho2 * self.hP_b1 + ((1 - rho2) * self.db1) * self.db1\n",
        "\n",
        "      nuPhat_b1 = self.nuP_b1/(1 - rho1**t)\n",
        "      hPhat_b1 = self.hP_b1/(1-rho2**t)\n",
        "\n",
        "      self.b1 = self.b1 - (alpha * nuPhat_b1)/(np.sqrt(hPhat_b1) + delta)\n",
        "\n",
        "      #iterate the time step\n",
        "      self.timeStep += 1\n",
        "\n",
        "    def train(self, training_data, num_epochs, batch_size, learning_rate):\n",
        "      start_time = time.time()\n",
        "\n",
        "      #transform the training data into a numpy\n",
        "      training_tensor = torch.tensor(training_data.values)\n",
        "\n",
        "      for epoch_idx in range(num_epochs):\n",
        "        print(f\"EPOCH: {epoch_idx + 1}\")\n",
        "\n",
        "        epoch = training_tensor[torch.randperm(training_tensor.size(0))] #generate a random permutation of X rows\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx in range(batch_size, epoch.size(0), batch_size):\n",
        "          #generate the minibatch\n",
        "          start, end = batch_idx - batch_size, batch_idx\n",
        "          X_minibatch, y_minibatch = epoch[start:end, 1:].float().numpy().T, epoch[start:end, 0]\n",
        "\n",
        "          #y_minibatch is just a tensor of labels, we need to one hot encode them as tensors\n",
        "          y_encoded = torch.nn.functional.one_hot(y_minibatch, 10).float().numpy().T\n",
        "\n",
        "          # Compute prediction and loss on the minibatch\n",
        "          self.forward(X_minibatch)\n",
        "\n",
        "          loss = self.loss(X_minibatch, y_encoded)\n",
        "          total_loss += loss\n",
        "\n",
        "          # Backpropagation\n",
        "          self.backward(y_encoded)\n",
        "          self.adam_step()\n",
        "\n",
        "          #print the batch loss\n",
        "          print(\"\\tminibatch {:<3d} LOSS: {:.2f}   [{:5d}/{:5d}]\".format(int (batch_idx/batch_size), loss, end, epoch.size(0)))\n",
        "\n",
        "        #print the average loss for the epoch\n",
        "        print(\"\\n\\tAVG. LOSS: {:.2f}\".format(total_loss/(epoch.size(0) // batch_size)))\n",
        "\n",
        "\n",
        "      end_time = time.time()\n",
        "      print(\"\\nTIME: {:.2f} sec\".format(end_time-start_time))\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "\n",
        "      #generate predictions and convert them to labels\n",
        "      y_preds_probs = model1.forward(X_test.numpy().T)\n",
        "      y_preds = np.argmax(y_preds_probs.T, axis=1)\n",
        "\n",
        "      #compute the accracy\n",
        "      test_accuracy = 100 * np.equal(y_test.numpy(), y_preds).sum()/y_test.size(0)\n",
        "      print(\"MODEL 1 ACCURACY: {:.2f}%\".format(test_accuracy))\n"
      ],
      "metadata": {
        "id": "OXTsAJeNdiVR"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to use minibatch for training in this project, because thhe size of my dataset included 42000 rows, I felt that doing stochastic and batch would be too slow, and minibatch would provide a good balance of accuracy and speed."
      ],
      "metadata": {
        "id": "tlfv0A82q-7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = NeurelNetwork(784, 1000, 1000, 10)\n",
        "model1.train(df_train, 6, 2000, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPuKtzgv6yQn",
        "outputId": "687aa8db-1fd1-48f0-87ad-f669f8cd6100"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-153-6163baad2e64>:113: RuntimeWarning: divide by zero encountered in log\n",
            "  f = lambda x, y : -(x * np.log(y) + (1 - x) * np.log(1 - y)) #negative log likely-hood\n",
            "<ipython-input-153-6163baad2e64>:113: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  f = lambda x, y : -(x * np.log(y) + (1 - x) * np.log(1 - y)) #negative log likely-hood\n",
            "<ipython-input-153-6163baad2e64>:113: RuntimeWarning: invalid value encountered in log\n",
            "  f = lambda x, y : -(x * np.log(y) + (1 - x) * np.log(1 - y)) #negative log likely-hood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "EPOCH: 2\n",
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "EPOCH: 3\n",
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "EPOCH: 4\n",
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "EPOCH: 5\n",
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "EPOCH: 6\n",
            "\tminibatch 1   LOSS: nan   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: nan   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: nan   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: nan   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: nan   [10000/33600]\n",
            "\tminibatch 6   LOSS: nan   [12000/33600]\n",
            "\tminibatch 7   LOSS: nan   [14000/33600]\n",
            "\tminibatch 8   LOSS: nan   [16000/33600]\n",
            "\tminibatch 9   LOSS: nan   [18000/33600]\n",
            "\tminibatch 10  LOSS: nan   [20000/33600]\n",
            "\tminibatch 11  LOSS: nan   [22000/33600]\n",
            "\tminibatch 12  LOSS: nan   [24000/33600]\n",
            "\tminibatch 13  LOSS: nan   [26000/33600]\n",
            "\tminibatch 14  LOSS: nan   [28000/33600]\n",
            "\tminibatch 15  LOSS: nan   [30000/33600]\n",
            "\tminibatch 16  LOSS: nan   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: nan\n",
            "\n",
            "TIME: 505.53 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seperate the testing data into features and target\n",
        "X_test, y_test = torch.tensor(df_test.values)[:, 1:].float(), torch.tensor(df_test.values)[:, 0]\n",
        "\n",
        "model1.test(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFPw2DZjQu15",
        "outputId": "d85f9d66-53d2-4ee8-dc16-f81b164bf02c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 1 ACCURACY: 82.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2\n",
        "\n",
        "##Task 1\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch\n",
        "\n",
        "These are two links that I used when I was constructing my neural network in pytorch. They First one\n",
        "pretty much covered how to create a neurel network as an object, and the second link provided the information\n",
        "I needed when it came to figuring out how to generate predicitions using that dataset.\n",
        "Any remaining research beyond these two linked pretty much came from browsing the pytorch\n",
        "library for various functions to make programming, especially the testing portion, easier.\n",
        "The link to the pytorch library is as follows...\n",
        "\n",
        "https://pytorch.org/docs/stable/index.html"
      ],
      "metadata": {
        "id": "Yog6yYB1qgoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2"
      ],
      "metadata": {
        "id": "4xU9ERnaqjaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "import time\n",
        "\n",
        "class NeuralNetwork2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(model, training_data, num_epochs, batch_size, learning_rate):\n",
        "  start_time = time.time()\n",
        "\n",
        "  #initalize the loss function and the optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  #transform the training data into a tensor\n",
        "  training_tensor = torch.tensor(training_data.values)\n",
        "\n",
        "  for epoch_idx in range(num_epochs):\n",
        "    print(f\"EPOCH: {epoch_idx + 1}\")\n",
        "\n",
        "    epoch = training_tensor[torch.randperm(training_tensor.size(0))] #generate a random permutation of X rows\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx in range(batch_size, epoch.size(0), batch_size):\n",
        "      #generate the minibatch\n",
        "      start, end = batch_idx - batch_size, batch_idx\n",
        "      X_minibatch, y_minibatch = epoch[start:end, 1:].float(), epoch[start:end, 0]\n",
        "\n",
        "      #y_minibatch is just a tensor of labels, we need to one hot encode them as tensors\n",
        "      y_encoded = torch.nn.functional.one_hot(y_minibatch, 10).float()\n",
        "\n",
        "      #zero the gradient and\n",
        "      optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
        "      y_preds = model(X_minibatch)\n",
        "\n",
        "      #compute the loss\n",
        "      loss = criterion(y_preds, y_encoded)\n",
        "      total_loss += loss\n",
        "\n",
        "      #preform backwards propogation and update the weights\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #print the batch loss\n",
        "      print(\"\\tminibatch {:<3d} LOSS: {:.2f}   [{:5d}/{:5d}]\".format(int (batch_idx/batch_size), loss, end, epoch.size(0)))\n",
        "\n",
        "    #print the average loss for the epoch\n",
        "    print(\"\\n\\tAVG. LOSS: {:.2f}\".format(total_loss/(epoch.size(0) // batch_size)))\n",
        "\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(\"\\nTIME: {:.2f} sec\".format(end_time-start_time))"
      ],
      "metadata": {
        "id": "L5pv-0-nooX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = NeuralNetwork2()\n",
        "train(model2, df_train, 10, 2000, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqIg4NzFyEWq",
        "outputId": "363bbd8f-aa5a-4eb3-c2ce-16dcd6bcf9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "\tminibatch 1   LOSS: 2.36   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 2.17   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 2.08   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 2.09   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 2.07   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.98   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.97   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.96   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.84   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.82   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.79   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.81   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.77   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.77   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.75   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.76   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.94\n",
            "EPOCH: 2\n",
            "\tminibatch 1   LOSS: 1.74   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.76   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.74   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.74   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.71   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.72   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.72   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.72   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.70   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.71   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.70   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.72   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.70   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.71   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.70   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.70   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.72\n",
            "EPOCH: 3\n",
            "\tminibatch 1   LOSS: 1.69   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.69   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.68   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.67   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.68   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.70   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.70   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.67   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.67   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.67   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.67   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.66   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.68   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.67   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.67   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.67   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.68\n",
            "EPOCH: 4\n",
            "\tminibatch 1   LOSS: 1.66   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.66   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.66   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.66   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.67   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.66   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.67   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.66   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.64   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.65   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.66   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.66   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.65   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.64   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.64   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.66   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.66\n",
            "EPOCH: 5\n",
            "\tminibatch 1   LOSS: 1.65   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.65   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.66   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.64   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.63   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.66   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.64   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.64   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.64   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.63   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.60   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.61   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.62   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.62   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.59   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.59   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.63\n",
            "EPOCH: 6\n",
            "\tminibatch 1   LOSS: 1.60   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.59   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.58   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.58   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.58   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.58   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.58   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.58   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.57   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.57   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.56   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.57   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.57   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.57   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.57   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.57   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.58\n",
            "EPOCH: 7\n",
            "\tminibatch 1   LOSS: 1.56   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.57   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.56   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.56   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.57   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.57   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.56   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.56   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.56   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.56   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.56   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.56   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.56   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.55   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.56   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.56   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.56\n",
            "EPOCH: 8\n",
            "\tminibatch 1   LOSS: 1.55   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.56   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.55   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.55   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.56   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.56   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.57   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.55   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.55   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.55   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.56   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.55   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.55   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.55   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.55   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.55   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.55\n",
            "EPOCH: 9\n",
            "\tminibatch 1   LOSS: 1.55   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.55   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.55   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.55   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.55   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.56   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.56   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.56   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.54   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.56   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.54   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.54   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.54   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.55   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.56   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.55   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.55\n",
            "EPOCH: 10\n",
            "\tminibatch 1   LOSS: 1.54   [ 2000/33600]\n",
            "\tminibatch 2   LOSS: 1.55   [ 4000/33600]\n",
            "\tminibatch 3   LOSS: 1.55   [ 6000/33600]\n",
            "\tminibatch 4   LOSS: 1.54   [ 8000/33600]\n",
            "\tminibatch 5   LOSS: 1.55   [10000/33600]\n",
            "\tminibatch 6   LOSS: 1.54   [12000/33600]\n",
            "\tminibatch 7   LOSS: 1.55   [14000/33600]\n",
            "\tminibatch 8   LOSS: 1.54   [16000/33600]\n",
            "\tminibatch 9   LOSS: 1.55   [18000/33600]\n",
            "\tminibatch 10  LOSS: 1.55   [20000/33600]\n",
            "\tminibatch 11  LOSS: 1.55   [22000/33600]\n",
            "\tminibatch 12  LOSS: 1.55   [24000/33600]\n",
            "\tminibatch 13  LOSS: 1.55   [26000/33600]\n",
            "\tminibatch 14  LOSS: 1.54   [28000/33600]\n",
            "\tminibatch 15  LOSS: 1.54   [30000/33600]\n",
            "\tminibatch 16  LOSS: 1.54   [32000/33600]\n",
            "\n",
            "\tAVG. LOSS: 1.55\n",
            "\n",
            "TIME: 58.90 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seperate the testing data into features and target\n",
        "X_test, y_test = torch.tensor(df_test.values)[:, 1:].float(), torch.tensor(df_test.values)[:, 0]\n",
        "\n",
        "#generate predictions and convert them to labels\n",
        "y_preds_probs = model2(X_test)\n",
        "y_preds = torch.argmax(y_preds_probs, dim=1)\n",
        "\n",
        "#compute the accracy\n",
        "test_accuracy = 100 * torch.eq(y_test, y_preds).sum()/y_test.size(0)\n",
        "print(\"MODEL 2 ACCURACY: {:.2f}%\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7Pk8DwsM1_Q",
        "outputId": "247d040e-d3cc-41e9-8669-f85181921fb4"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 2 ACCURACY: 91.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "\n",
        "When it came to selecting the hyperparameters I went ahead and used standard stochastic gradient descent\n",
        "mainly because I wanted to see how it contrasted to my implementation of the adam optimized. For the learning rate I\n",
        "just chose 0.01 as it is pretty much the standard learning rate, at least according to most sources online and my\n",
        "previous experience implementing neural networks in other classes. For the dimension of the two hidden layers, I used\n",
        "1000 twice because I thought that that the hidden layers should have a dimension higher than the number of pixels to\n",
        "try and capture more complexity than would be contained in just the pixels. As for activation functions, I used ReLu and\n",
        "Sigmoid. My hope with ReLu was that it would alleviate any problem with vanishing gradient. As for sigmoid, because I was\n",
        "running a classication problem, I needed the sigmoid function in order to return probabilities. For this project I did not\n",
        "use regularization because my initial implementation did not incorporate it and ended up preforming fine. I also did not want\n",
        "to risk the added time of preforming regularization. Finally, while it is not a hyperperameter, I used minibatch on the training\n",
        "because of large size of the dataset, my hope was that it would minimize the time it took to train the model, while balancing\n",
        "accuracy."
      ],
      "metadata": {
        "id": "Wa883lh4qzBh"
      }
    }
  ]
}