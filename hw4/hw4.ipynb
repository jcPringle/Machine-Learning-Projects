{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uDVEwyHyuZL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124256cc-181b-4ce5-f401-507fcaf40454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: because the original datset was so large, approximately three million instances in the traing set, I will be instead sampling from the smaller test set (650,000 instances). Avoiding loading the training set saves me much needed memory. The sample I take from the testing set will only be 10000 entries, as this is what I have determined to be the best when balancing memory usage. I have however ensured that the distribution of labels within the sampled set is even."
      ],
      "metadata": {
        "id": "W7_BqsFlksH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading the dataset\n",
        "\n",
        "import pandas as pd\n",
        "column_headers = ['Rating', 'Title', 'Review']\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/amazon_sentiment_analysis/test.csv', header=None, names=column_headers)"
      ],
      "metadata": {
        "id": "d5FIpNCI1JLf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis and Processing\n"
      ],
      "metadata": {
        "id": "eb9oZ8UP2E78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def labelDistribution(df, label_name):\n",
        "  for l in df[label_name].unique():\n",
        "    n = df[label_name].value_counts()[l]\n",
        "    print(\"{}: {:.2f}%\".format(l, n/df.shape[0] * 100))"
      ],
      "metadata": {
        "id": "UcnGxVADG0WF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def sample(df, label_name, feature_name, sample_size):\n",
        "  #Find the frequency of each rating\n",
        "  label_freq = df[label_name].value_counts()\n",
        "\n",
        "  # Calculate the sample size for each class based on the desired sample size\n",
        "  class_sample_size = {label: min(sample_size, freq) for label, freq in label_freq.items()}\n",
        "\n",
        "  #this is the proportion of the data that will be taken for the sample\n",
        "  test_size = 1 - (sample_size / df.shape[0])\n",
        "\n",
        "  # Perform sampling stratified by label\n",
        "  sample_indexes = StratifiedShuffleSplit(n_splits=1, test_size = test_size, random_state=17)\n",
        "\n",
        "  # sampled_data = None\n",
        "  for train_idx, _ in sample_indexes.split(df[feature_name], df[label_name]):\n",
        "      sampled_df = df.iloc[train_idx]\n",
        "\n",
        "  return sampled_df"
      ],
      "metadata": {
        "id": "SL2_MdIbwWrE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample a subset from the full dataset because the dataset is too large\n",
        "df = sample(df, 'Rating', 'Review', 10000)"
      ],
      "metadata": {
        "id": "3pFiP6A7JZSB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#determine the number of null entries in the dataset\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49Dx-yflpwQ",
        "outputId": "b971feba-3597-4ff3-ac05-23208b9fc9b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rating    0\n",
              "Title     0\n",
              "Review    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data Distribution:\\n')\n",
        "labelDistribution(df, 'Rating')"
      ],
      "metadata": {
        "id": "bX6YP6pYJ8dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd967af-ecb5-47fc-a834-311d3d69538d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Distribution:\n",
            "\n",
            "1: 20.00%\n",
            "4: 20.00%\n",
            "2: 20.00%\n",
            "3: 20.00%\n",
            "5: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Autoencoder\n",
        "\n"
      ],
      "metadata": {
        "id": "KhSO8e9TTGn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "#initlaize the size of the embeddings\n",
        "vector_size = 75\n",
        "\n",
        "#isolate the text into its own dataframe\n",
        "df_copy = df[['Rating','Review']].copy()\n",
        "print(\"\\rDATA LOADED\", end = '')\n",
        "\n",
        "#tokenize each of the reviews in the dataset\n",
        "tokenized_data = df_copy.Review.apply(gensim.utils.simple_preprocess)\n",
        "print(\"\\rDATA TOKENIZED\", end = '')\n",
        "\n",
        "#initialize a word2Vec encoder model\n",
        "autoencoder = gensim.models.Word2Vec(min_count=1, vector_size=vector_size, window=7, workers=10)\n",
        "print(\"\\rAUTOENCODER INTIALIZED\", end = '')\n",
        "\n",
        "#train the model on the tokenized data\n",
        "autoencoder.build_vocab(tokenized_data, progress_per=1000) #constructs vocabulary\n",
        "print(\"\\rVOCABULARY CONSTRUCTED\", end = '')\n",
        "\n",
        "autoencoder.train(tokenized_data, total_examples=autoencoder.corpus_count, epochs=3) #corpus_count: # sentences, model.epochs: # epochs\n",
        "print(\"\\rAUTOENCODER TRAINING COMPLETE\", end = '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxq4gLcia-nM",
        "outputId": "8ab6f3d3-c46a-438f-bf03-8d19770ae05c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTOENCODER TRAINING COMPLETE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "#find the maximum length sequence in the dataset as determined by the number of words\n",
        "\n",
        "def encode(tokenized_data, vector_size, encoder, max_len=-1):\n",
        "  #find how many instances there are in the dataset\n",
        "  instances = tokenized_data.shape[0]\n",
        "\n",
        "  #compute the maximum number of tokens in a given instance\n",
        "  if(max_len == -1):\n",
        "    max_len = len(max(tokenized_data, key=len))\n",
        "\n",
        "  #generate embeddings\n",
        "  encoded_data = np.zeros((instances, max_len, vector_size))\n",
        "\n",
        "  #initlaize the padding and embedding outside of the loop to reduce memory consumption\n",
        "  padding = None\n",
        "  raw_embedding = None\n",
        "  for i in range(instances):\n",
        "    print(\"\\rinstances encoded: {}\".format(i+1), end = '')\n",
        "\n",
        "    #calculate the padding and raw embedding for the sequence\n",
        "    padding = np.zeros((max_len - len(tokenized_data.iloc[i]), vector_size))\n",
        "    raw_embedding = np.array([encoder.wv[token] for token in tokenized_data.iloc[i]])\n",
        "\n",
        "    #add the padding to the raw embedding before incorporating it into enocded_data\n",
        "    if(len(padding) == max_len):\n",
        "      encoded_data[i] = padding\n",
        "    else:\n",
        "      encoded_data[i] = np.concatenate((raw_embedding, padding), axis=0)\n",
        "\n",
        "  return encoded_data"
      ],
      "metadata": {
        "id": "WYKB5VNXCZET"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for seperating the training and testing datasets\n",
        "ratio = 0.8\n",
        "n = int(ratio * len(df))\n",
        "\n",
        "#this is the maxium sequence size in the entire tokenized dataset\n",
        "max_len = len(max(tokenized_data, key=len))\n",
        "\n",
        "X_train, y_train = torch.FloatTensor(encode(tokenized_data[:n], vector_size, autoencoder, max_len)), torch.tensor(df.Rating[:n].values)\n",
        "X_test, y_test = torch.FloatTensor(encode(tokenized_data[n:], vector_size, autoencoder, max_len)), torch.tensor(df.Rating[n:].values)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOBSs5VoDH3U",
        "outputId": "dd310761-d019-4e55-993c-971bbac75048"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instances encoded: 2000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decrement the ratings by 1 and one hot encode them\n",
        "y_train = torch.nn.functional.one_hot(y_train-1, 5).float()\n",
        "y_test = torch.nn.functional.one_hot(y_test-1, 5).float()"
      ],
      "metadata": {
        "id": "NmJBJHLqJ6-z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2"
      ],
      "metadata": {
        "id": "Ux1HtgrUKHQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Description\n",
        "\n",
        "NOTE: This model, as you will see, is extremely innaccurate. I have tried fixing this in a number of ways, including modifying all the hyperparameters and double checking the data. That all being said, unless I have missed something, I have come to the conclusion that this innaccuracy may be the result of the model simply not being complex enough for the task at hand. Unfortunately due the limitations of my hardward, I cannot not increase the complexity of my model without also crashing my system, I have tried though.\n",
        "\n",
        "Because I am only classifying text, in this case reviews, into one of five categories each of which represents a customer rating ranging from one to five starts, I went ahead and used a many to one RNN. In this scenario, I am passing in a sequence of word embeddings, which were generated from the reviews. The output of the RNN is a vector of size five. This final layer has a softmax applied to it before it is returned so that each value in the vector represents a probability corresponding to a rating. At each timestep, a tanh activation is applied to the output of each cell. Because my loss was remaining the same across epochs, I initially tried ReLU. My hope was that the stagnation in loss was a result of a vanishing gradient. However, this did nothing really to solve my problem, so I moved to tanh. Tanh did not really fix the problem but it did demonstrate lower loss in general than ReLU so I stuck with it as my activation function.  \n"
      ],
      "metadata": {
        "id": "gINVcPe3jngI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#because this is sentiment analysis, this will have to be a many in single out RNN\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out, layers):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.input_dim = dim_in #the size of the embedding\n",
        "    self.hidden_dim = dim_h #the size of the hidden layer\n",
        "    self.output_dim = dim_out #the number of classifiers\n",
        "\n",
        "    self.num_layers = layers #the number of layers in the RNN\n",
        "\n",
        "    #The RNN itself\n",
        "    self.rnn = nn.RNN(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, nonlinearity = 'tanh')\n",
        "\n",
        "    #Intialize the output layer\n",
        "    self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    #Initlaize the first hidden layer based on the size of the batch\n",
        "    h0 = torch.rand(self.num_layers, X.size(0), self.hidden_dim)\n",
        "\n",
        "    #iterate one time step forward\n",
        "    out, hn = self.rnn(X, h0)\n",
        "    out = self.output_layer(out)[:, -1, :]\n",
        "\n",
        "    #apply a softmax to the raw output\n",
        "    out = torch.nn.functional.softmax(out, dim = 1)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "dvIW3f9iTFt_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out, layers):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.input_dim = dim_in #the size of the embedding\n",
        "    self.hidden_dim = dim_h #the size of the hidden layer\n",
        "    self.output_dim = dim_out #the number of classifiers\n",
        "\n",
        "    self.num_layers = layers #the number of layers in the RNN\n",
        "\n",
        "    #The RNN itself\n",
        "    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "\n",
        "    #Intialize the output layer\n",
        "    self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    #Initlaize the first hidden layer and cell state\n",
        "    h0 = torch.rand(self.num_layers, X.size(0), self.hidden_dim)\n",
        "    c0 = torch.rand(self.num_layers, X.size(0), self.hidden_dim)\n",
        "\n",
        "    #iterate one time step forward\n",
        "    out, hn = self.lstm(X, (h0, c0))\n",
        "    out = self.output_layer(out)[:, -1, :]\n",
        "\n",
        "    #apply a softmax to the raw output\n",
        "    out = torch.nn.functional.softmax(out, dim = 1)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "MhI5WLu-pphb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, dim_in, dim_h, dim_out, layers):\n",
        "    super(GRU, self).__init__()\n",
        "\n",
        "    self.input_dim = dim_in #the size of the embedding\n",
        "    self.hidden_dim = dim_h #the size of the hidden layer\n",
        "    self.output_dim = dim_out #the number of classifiers\n",
        "\n",
        "    self.num_layers = layers #the number of layers in the RNN\n",
        "\n",
        "    #The RNN itself\n",
        "    self.gru = nn.GRU(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "\n",
        "    #Intialize the output layer\n",
        "    self.output_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    #Initlaize the first hidden layer and cell state\n",
        "    h0 = torch.rand(self.num_layers, X.size(0), self.hidden_dim)\n",
        "\n",
        "    #iterate one time step forward\n",
        "    out, hn = self.gru(X, h0)\n",
        "\n",
        "    out = self.output_layer(out)[:, -1, :]\n",
        "\n",
        "    #apply a softmax to the raw output\n",
        "    out = torch.nn.functional.softmax(out, dim = 1)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "a_ZGEJ8ctzxv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n",
        "  start_time = time.time()\n",
        "\n",
        "  #initalize the loss function and the optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  #transform the training data into tensor\n",
        "  training_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "  for epoch_idx in range(num_epochs):\n",
        "    print(f\"EPOCH: {epoch_idx + 1}\")\n",
        "\n",
        "    epoch = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (X_minibatch, y_minibatch) in enumerate(epoch):\n",
        "\n",
        "      #zero the gradient and\n",
        "      optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
        "      y_preds = model(X_minibatch)\n",
        "\n",
        "      #compute the loss\n",
        "      loss = criterion(y_preds, y_minibatch)\n",
        "      total_loss += loss\n",
        "\n",
        "      #preform backwards propogation and update the weights\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #print the batch loss\n",
        "      print(\"\\tminibatch {:<3d} LOSS: {:.6f}   [{:5d}/{:5d}]\".format(batch_idx, loss, (batch_idx + 1) * batch_size, X_train.size(0)))\n",
        "\n",
        "    #print the average loss for the epoch\n",
        "    print(\"\\n\\tAVG. LOSS: {:.6f}\".format(total_loss/(X_train.size(0) // batch_size)))\n",
        "\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(\"\\nTIME: {:.2f} sec\".format(end_time-start_time))"
      ],
      "metadata": {
        "id": "ohxdKIoSiozv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the models\n",
        "model_RNN = RNN(vector_size, 125, 5, 1)\n",
        "model_LSTM = LSTM(vector_size, 125, 5, 1)\n",
        "model_GRU = GRU(vector_size, 125, 5, 1)"
      ],
      "metadata": {
        "id": "bk098HjXhemM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the basic RNN\n",
        "train(model_RNN, X_train, y_train, 4, 1000, 0.01)"
      ],
      "metadata": {
        "id": "9gsuhkuSiotv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2905dfd3-d3d7-4696-8662-38af38ad653e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "\tminibatch 0   LOSS: 1.609866   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.610573   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609855   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.613003   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.613163   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.629937   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.613784   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.610171   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.613794\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.616458   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.618359   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.613925   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.617797   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.611846   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.617542   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.613588   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.618344   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.615983\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.620070   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.615829   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.616445   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.610554   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.618730   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.611001   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.617291   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.610663   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.615073\n",
            "EPOCH: 4\n",
            "\tminibatch 0   LOSS: 1.609245   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.612026   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.613511   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.611627   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.612118   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609444   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.611069   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.617608   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.612081\n",
            "\n",
            "TIME: 118.21 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the LMST model\n",
        "train(model_LSTM, X_train, y_train, 4, 1000, 0.01)"
      ],
      "metadata": {
        "id": "x15Gr1JFs-aO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4cb8fc4-3de6-4252-e64c-c00841819fd7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "\tminibatch 0   LOSS: 1.609538   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.610042   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609119   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.611339   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.610391   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609948   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609818   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609499   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609962\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.609586   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.609543   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609465   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.609315   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.609344   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609257   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.610003   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609361   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609484\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.609243   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.609559   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609758   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.609546   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.609515   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.608813   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.610020   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609344   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609475\n",
            "EPOCH: 4\n",
            "\tminibatch 0   LOSS: 1.609258   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.609909   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609264   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.609862   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.608743   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609116   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609550   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609287   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609373\n",
            "\n",
            "TIME: 1113.12 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the GRU model\n",
        "train(model_GRU, X_train, y_train, 4, 1000, 0.01)"
      ],
      "metadata": {
        "id": "F3F4s7iVtAa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a0e9c0-2a32-40c4-c58a-86964418a208"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "\tminibatch 0   LOSS: 1.609718   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.609511   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.610316   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.610785   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.610297   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609922   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609207   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609796   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609944\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.609279   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.610250   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609377   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.609338   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.609563   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609644   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609384   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.610492   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609666\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.608796   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.608806   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609390   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.610155   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.609710   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609553   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609271   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609653   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609417\n",
            "EPOCH: 4\n",
            "\tminibatch 0   LOSS: 1.609224   [ 1000/ 8000]\n",
            "\tminibatch 1   LOSS: 1.609292   [ 2000/ 8000]\n",
            "\tminibatch 2   LOSS: 1.609329   [ 3000/ 8000]\n",
            "\tminibatch 3   LOSS: 1.609130   [ 4000/ 8000]\n",
            "\tminibatch 4   LOSS: 1.609580   [ 5000/ 8000]\n",
            "\tminibatch 5   LOSS: 1.609134   [ 6000/ 8000]\n",
            "\tminibatch 6   LOSS: 1.609431   [ 7000/ 8000]\n",
            "\tminibatch 7   LOSS: 1.609351   [ 8000/ 8000]\n",
            "\n",
            "\tAVG. LOSS: 1.609309\n",
            "\n",
            "TIME: 220.83 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "As for my metrics, I am using three. My first metric is just regular accuracy which serves as the baseline to communicate how my model is doing overall. Beyond that, I am also comparing the training time for each model. This is mainly to keep track of the difference between the three models I am contracting in terms of their complexity. Finally, I would like to say that it was my original intent to use the area under the ROC curve as another metric, but unfortunately this ended up requiring an extraordinary amount of memory, so this metric was scrapped at the last minute. My hope was to determine my modelâ€™s confidence in its choices using this metric. Though the actual processing has been left out of the notebook, the function for computing the ROC area is still there.\n"
      ],
      "metadata": {
        "id": "ahuA9XkZjg8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def accuracy(model, X_test, y_test):\n",
        "  #generate predictions and convert them to labels\n",
        "  y_preds = model(X_test) #probabilites\n",
        "  y_preds = torch.argmax(y_preds, dim=1)\n",
        "\n",
        "  #compute the accuracy\n",
        "  return 100 * torch.eq(torch.argmax(y_test, dim=1), y_preds).sum()/y_test.size(0)\n",
        "\n",
        "def roc(model, X_test, y_test):\n",
        "  #generate predictions and convert them to labels\n",
        "  y_preds = model(X_test) #probabilites\n",
        "  _, y_preds = torch.max(y_preds, 1) #find the highest probabilities\n",
        "  y_preds = y_preds.detach().numpy()\n",
        "\n",
        "  print(y_preds)\n",
        "\n",
        "  #compute the accuracy\n",
        "  return roc_auc_score(y_test, y_preds)"
      ],
      "metadata": {
        "id": "NJFHnx-JwJVl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the accuracy for each model for the train set\n",
        "accuracy_RNN_train = accuracy(model_RNN, X_train, y_train)\n",
        "accuracy_LSTM_train = accuracy(model_LSTM, X_train, y_train)\n",
        "accuracy_GRU_train = accuracy(model_GRU, X_train, y_train)\n",
        "\n",
        "#display the results\n",
        "print(\"TRAIN:\\nBASIC RNN ACCURACY: {:.2f}%\\nLSTM RNN ACCURACY: {:.2f}%\\nGRU RNN ACCURACY: {:.2f}%\".format(accuracy_RNN_train,\n",
        "                                                                                                              accuracy_LSTM_train,\n",
        "                                                                                                              accuracy_GRU_train))\n",
        "#find the accuracy for each model for the test set\n",
        "accuracy_RNN_test = accuracy(model_RNN, X_test, y_test)\n",
        "accuracy_LSTM_test = accuracy(model_LSTM, X_test, y_test)\n",
        "accuracy_GRU_test = accuracy(model_GRU, X_test, y_test)\n",
        "\n",
        "#display the results\n",
        "print(\"\\nVALIDATION:\\nBASIC RNN ACCURACY: {:.2f}%\\nLSTM RNN ACCURACY: {:.2f}%\\nGRU RNN ACCURACY: {:.2f}%\".format(accuracy_RNN_test,\n",
        "                                                                                                              accuracy_LSTM_test,\n",
        "                                                                                                              accuracy_GRU_test))"
      ],
      "metadata": {
        "id": "Fy5qhALxuowy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f438a7e-0211-4d1f-e545-402e0a0b93e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN:\n",
            "BASIC RNN ACCURACY: 20.05%\n",
            "LSTM RNN ACCURACY: 20.33%\n",
            "GRU RNN ACCURACY: 20.30%\n",
            "\n",
            "VALIDATION:\n",
            "BASIC RNN ACCURACY: 19.80%\n",
            "LSTM RNN ACCURACY: 18.90%\n",
            "GRU RNN ACCURACY: 18.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations about LSTM and GRU\n",
        "After running the LSTM and GRU RNNs the main thing that noticed was difference in terms of how long it took for each to train. While the basic RNN and GRU took 118 and 220 seconds respectively, the LSTM took much longer, finishing at roughly 1100 seconds or 18 minutes. In terms of accuracy, they all preformed roughly the same, which in this case is poorly. As stated before however, this is more likely a result of the limited complexity of the models when compared to the problem, and the limitations of my hardware. As for why it took the LSTM and GRU RNNs longer, both processes complicate the standard RNN by incorporating gates. The difference is that GRU incorporates significantly few operations than LSTM which would probably explain why it took so much longer.\n"
      ],
      "metadata": {
        "id": "fhuvSRcCja4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Could you use a feed-forward network?\n",
        "For this problem you almost certainly could not you a standard feed-forward network.  This is because the order of the words in the text is a feature of the data that your model needs to capture. Depending on how the words are arranged, the resulting rating could be completely different. This kind of sequential information cannot be capture by a traditional feed-forward network, and this is why a sequential model like an RNN is necessary.\n",
        "\n"
      ],
      "metadata": {
        "id": "buiZk3PkjSav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3"
      ],
      "metadata": {
        "id": "Jxm6vf2kJ3_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity/Dissimilarity\n",
        "Ok I just used the standard cosine similarity and dissimilarity score equation. That is defined by\n",
        "\n",
        "$$  \\frac{A \\cdot B}{\\left\\lVert A \\right\\rVert \\left\\lVert B \\right\\rVert } $$\n",
        "\n",
        "and\n",
        "\n",
        "$$  1 - \\frac{A \\cdot B}{\\left\\lVert A \\right\\rVert \\left\\lVert B \\right\\rVert } $$\n",
        "\n",
        "This is not something that I got from a paper so much as it is something that I just know from this class and prior classes.\n"
      ],
      "metadata": {
        "id": "TaZf40JFjMCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#computes the cosine similarity and dissimilarity and returns them\n",
        "def similiarity(encoder, word1, word2):\n",
        "  #generate the emebeddings for each word and store them as tensors\n",
        "  embedding1 = encoder.wv[word1.lower()]\n",
        "  embedding2 = encoder.wv[word2.lower()]\n",
        "\n",
        "  #compute the dot product\n",
        "  numerator = embedding1 @ embedding2.T\n",
        "\n",
        "  #compute the products of the euclidian norms of each embedding\n",
        "  denominator = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n",
        "\n",
        "  similarity = numerator/denominator\n",
        "\n",
        "  return similarity, 1-similarity"
      ],
      "metadata": {
        "id": "HFcThniwCLNZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "word1 = input(\"ENTER WORD 1: \")\n",
        "word2 = input(\"ENTER WORD 2: \")\n",
        "\n",
        "similarity, dissimilarity = similiarity(autoencoder, word1, word2)\n",
        "\n",
        "print(\"\\nCOSINE SIMILARITY: {:.6f}\\nCOSINE DISSIMILARITY: {:.6f}\".format(similarity, dissimilarity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJNdPWaXGJkS",
        "outputId": "c958c4a4-e1b5-4eed-c6ef-5dedefba3f19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTER WORD 1: plastic\n",
            "ENTER WORD 2: good\n",
            "\n",
            "COSINE SIMILARITY: 0.759757\n",
            "COSINE DISSIMILARITY: 0.240243\n"
          ]
        }
      ]
    }
  ]
}