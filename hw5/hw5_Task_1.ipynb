{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDVEwyHyuZL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53b2c76-ce43-4598-a95f-52e267d379c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data: https://www.kaggle.com/datasets/puneet6060/intel-image-classification/data"
      ],
      "metadata": {
        "id": "p4Hf45jlBZzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Description\n",
        "\n",
        "For this problem I am going to running classification on an intel image dataset sourced from kaggle. The images fall into six categories; buildings, forests, glaciers, mountains, seas, and streets. The full dataset contains roughly 25,000 color images of size 150x150. Due to memory limitations we are going to be down sampling from the full set. The Training set will include 80 images from each of the six, while the testing set will only include 10 from each category. This would be higher, but for the ResNet-50 model this seems to be just about as high as I can go without exceeding the 12.7 GB memory cap."
      ],
      "metadata": {
        "id": "3NrvejPqoEVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis and Processing\n"
      ],
      "metadata": {
        "id": "eb9oZ8UP2E78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import glob as gb\n",
        "import cv2"
      ],
      "metadata": {
        "id": "uII9rsgxAKyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'drive/MyDrive/Datasets/intel_image_dataset/seg_train/'\n",
        "test_path = 'drive/MyDrive/Datasets/intel_image_dataset/seg_test/'\n",
        "pred_path = 'drive/MyDrive/Datasets/intel_image_dataset/seg_pred/'"
      ],
      "metadata": {
        "id": "MWDziaeM__DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = {'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}"
      ],
      "metadata": {
        "id": "JwH2255fSBpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linked below is a notebook. Some of the code relating to extracting the dataset and\n",
        "# converting it into a csv file was modified and the used for the save of this project\n",
        "\n",
        "# https://www.kaggle.com/code/offwitt0/intel-image-classification\n",
        "\n",
        "training_instances = {}\n",
        "training_total = 0\n",
        "\n",
        "testing_instances = {}\n",
        "testing_total = 0\n",
        "\n",
        "#find how many instances of each label are there in each dataset\n",
        "for folder in  os.listdir(train_path + 'seg_train') :\n",
        "  files = gb.glob(pathname= str( train_path +'seg_train//' + folder + '/*.jpg'))\n",
        "  found = len(files)\n",
        "\n",
        "  training_instances[folder] = found\n",
        "  training_total += found\n",
        "\n",
        "for folder in  os.listdir(test_path +'seg_test') :\n",
        "  files = gb.glob(pathname= str( test_path +'seg_test//' + folder + '/*.jpg'))\n",
        "  found = len(files)\n",
        "\n",
        "  testing_instances[folder] = found\n",
        "  testing_total += found\n",
        "\n",
        "for name in label.keys():\n",
        "  label_percentage_train = 100 * training_instances[name]/training_total\n",
        "  label_percentage_test = 100 * testing_instances[name]/testing_total\n",
        "\n",
        "  print('{}:\\n\\ttraining: {:.2f}%\\n\\ttest: {:.2f}%'.format(name, label_percentage_train, label_percentage_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrNB2sdEAJGg",
        "outputId": "c30b49a9-8e96-4fc9-e28d-6f3ec856b7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buildings:\n",
            "\ttraining: 15.61%\n",
            "\ttest: 14.56%\n",
            "forest:\n",
            "\ttraining: 16.18%\n",
            "\ttest: 15.79%\n",
            "glacier:\n",
            "\ttraining: 17.13%\n",
            "\ttest: 18.42%\n",
            "mountain:\n",
            "\ttraining: 17.90%\n",
            "\ttest: 17.55%\n",
            "sea:\n",
            "\ttraining: 16.20%\n",
            "\ttest: 16.99%\n",
            "street:\n",
            "\ttraining: 16.97%\n",
            "\ttest: 16.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dims = {(150, 150, 3): 0, 'other': 0}\n",
        "testing_dims = {(150, 150, 3): 0, 'other': 0}\n",
        "\n",
        "for folder in  os.listdir(train_path +'seg_train') :\n",
        "  files = gb.glob(pathname= str( train_path +'seg_train//' + folder + '/*.jpg'))\n",
        "  for file in files:\n",
        "    image = plt.imread(file)\n",
        "\n",
        "    if(image.shape == (150, 150, 3)):\n",
        "      training_dims[image.shape] += 1\n",
        "    else:\n",
        "      training_dims['other'] += 1\n",
        "\n",
        "for folder in  os.listdir(test_path +'seg_test') :\n",
        "  files = gb.glob(pathname= str( test_path +'seg_test//' + folder + '/*.jpg'))\n",
        "  for file in files:\n",
        "    image = plt.imread(file)\n",
        "\n",
        "    if(image.shape == (150, 150, 3)):\n",
        "      testing_dims[image.shape] += 1\n",
        "    else:\n",
        "      testing_dims['other'] += 1\n",
        "\n",
        "print(f'training dimensions: {training_dims}')\n",
        "print(f'testing dimensions: {testing_dims}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rl9vhkZ2H29",
        "outputId": "0c72c535-fcfa-4a82-d099-3fad36407b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dimensions: {(150, 150, 3): 13986, 'other': 48}\n",
            "testing dimensions: {(150, 150, 3): 2995, 'other': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as func\n",
        "\n",
        "def load(path, folder_name, sample_size):\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  # fill lists\n",
        "  for folder in  os.listdir(path + folder_name) :\n",
        "    files = gb.glob(pathname= str( path + folder_name + '//' + folder + '/*.jpg'))\n",
        "\n",
        "    ctr = 0\n",
        "    for file in files:\n",
        "      image = cv2.imread(file) #converts the image into a numpy array\n",
        "\n",
        "      # dont include images whose dimensions do not match (150, 150, 3)\n",
        "      if(image.shape != (150, 150, 3)):\n",
        "        continue\n",
        "      else:\n",
        "        ctr += 1\n",
        "\n",
        "      X.append(list(image))\n",
        "      y.append(label[folder])\n",
        "\n",
        "      if(ctr == sample_size):\n",
        "        break\n",
        "\n",
        "      print(\"\\rCATEGORY: {:.9s} ............... {:.2f}%\".format(folder, 100 * ctr/sample_size), end = '')\n",
        "\n",
        "  return torch.tensor(X).float(), func.one_hot(torch.tensor(y), 6).float()"
      ],
      "metadata": {
        "id": "IoDJdHFzglWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# convert the X_train and y_train to numpy arrays\n",
        "X_train, y_train = load(train_path, 'seg_train', 80)\n",
        "X_test, y_test = load(test_path, 'seg_test', 10)\n",
        "\n",
        "print(f'\\rX_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjrhPe8CTLTM",
        "outputId": "87573a10-ad21-48fd-8341-6166bc91a399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([480, 150, 150, 3]), y_train shape: torch.Size([480, 6])\n",
            "X_test shape: torch.Size([60, 150, 150, 3]), y_test shape: torch.Size([60, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "B18gFJU0nvba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # convolutional layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # pooling layers\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(128 * 18 * 18, 512)  # 18x18 is the spatial size after three max-pooling layers\n",
        "    self.fc2 = nn.Linear(512, 6)  # because there are siz categories\n",
        "\n",
        "  def forward(self, X):\n",
        "    # first convolutional layer\n",
        "    c1 = func.relu(self.conv1(X));\n",
        "    p1 = self.pool(c1)\n",
        "\n",
        "    # second convolutional layer\n",
        "    c2 = func.relu(self.conv2(p1));\n",
        "    p2 = self.pool(c2)\n",
        "\n",
        "    # third convolutional layer\n",
        "    c3 = func.relu(self.conv3(p2));\n",
        "    p3 = self.pool(c3)\n",
        "\n",
        "    # flattened output of the coonvolutional layers\n",
        "    co = torch.flatten(p3, 1) # flatten the dimensions\n",
        "\n",
        "    # fully connected layers\n",
        "    o1 = func.relu(self.fc1(co))\n",
        "    o2 = func.softmax(self.fc2(o1))\n",
        "\n",
        "    return o2"
      ],
      "metadata": {
        "id": "_ZQ9i4rSjbX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train(model, X_train, y_train, num_epochs, batch_size, learning_rate):\n",
        "  start_time = time.time()\n",
        "\n",
        "  #initalize the loss function and the optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  #transform the training data into tensor\n",
        "  training_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "  for epoch_idx in range(num_epochs):\n",
        "    print(f\"EPOCH: {epoch_idx + 1}\")\n",
        "\n",
        "    epoch = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (X_minibatch, y_minibatch) in enumerate(epoch):\n",
        "\n",
        "      #zero the gradient and\n",
        "      optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
        "      y_preds = model(X_minibatch)\n",
        "\n",
        "      #compute the loss\n",
        "      loss = criterion(y_preds, y_minibatch)\n",
        "      total_loss += loss\n",
        "\n",
        "      #preform backwards propogation and update the weights\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #print the batch loss\n",
        "      print(\"\\tminibatch {:<3d} LOSS: {:.6f}   [{:5d}/{:5d}]\".format(batch_idx, loss, (batch_idx + 1) * batch_size, X_train.size(0)))\n",
        "\n",
        "    #print the average loss for the epoch\n",
        "    print(\"\\n\\tAVG. LOSS: {:.6f}\".format(total_loss/(X_train.size(0) // batch_size)))\n",
        "\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(\"\\nTIME: {:.2f} sec\".format(end_time-start_time))"
      ],
      "metadata": {
        "id": "yNz7QwbP1IRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the convolutional networks\n",
        "cnn1 = CNN()"
      ],
      "metadata": {
        "id": "FB7zpnwL06_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the neural network)\n",
        "train(cnn1, X_train.permute(0, 3, 1, 2), y_train, 3, 48, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsgics8W25YZ",
        "outputId": "510bddfa-34e3-4224-8606-e1008e8c7499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-13683a255f41>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  o2 = func.softmax(self.fc2(o1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tminibatch 0   LOSS: 1.896871   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.918592   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.918592   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.897758   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.918592   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.981092   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.751925   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.856092   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.731092   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.939425   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.881003\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.897758   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.856092   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.876925   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.876925   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.793592   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.835258   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.918592   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.939425   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.856092   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.918592   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.876925\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.897758   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.814425   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.897758   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.918592   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.939425   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.835258   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.856092   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.856092   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.897758   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.856092   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.876925\n",
            "\n",
            "TIME: 106.94 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "For this homework I am just going to be using a normal accuracy measurement for my main metric. There are a couple of reasons for this. First of all the normal accuracy measurement is a good overview of how model is doing in general. Beyond that, the normal accuracy measurment is not very computationally intesive. This is specifically important because I am running resnet50, memory is at a premium and due to how long it already takes to train that model, I would prefer to keep minimize time to compute when possible. I would also like to use an roc_auc measurement to see what my models confidence is like but unfortunately past experience has told that this would almost certainly crash my notebook. Another metric that I might implement if I had some more time and memory would be a confusion matrix just to specifically point to which labels my model tends to get right.\n",
        "\n",
        "Additionally I will be measuring and comparing the time in seconds that it takes each model to complete it training. This is something I do for pretty much model as a way of guaging effeciency."
      ],
      "metadata": {
        "id": "9sycxaa82n1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def accuracy(model, X_test, y_test):\n",
        "  #generate predictions and convert them to labels\n",
        "  y_preds = model(X_test) #probabilites\n",
        "  y_preds = torch.argmax(y_preds, dim=1)\n",
        "\n",
        "  #compute the accuracy\n",
        "  return 100 * torch.eq(torch.argmax(y_test, dim=1), y_preds).sum()/y_test.size(0)\n",
        "\n",
        "def roc(model, X_test, y_test):\n",
        "  #generate predictions and convert them to labels\n",
        "  y_preds = model(X_test) #probabilites\n",
        "  _, y_preds = torch.max(y_preds, 1) #find the highest probabilities\n",
        "  y_preds = y_preds.detach().numpy()\n",
        "\n",
        "  print(y_preds)\n",
        "\n",
        "  #compute the accuracy\n",
        "  return roc_auc_score(y_test, y_preds)"
      ],
      "metadata": {
        "id": "N9VbqmzSbCfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Convolutional Neural Network 1\\n\")\n",
        "\n",
        "#find the accuracy for each model for the train set\n",
        "accuracy_cnn_train = accuracy(cnn1, X_train.permute(0, 3, 1, 2), y_train)\n",
        "\n",
        "#display the results\n",
        "print(\"TRAINING ACCURACY: {:.2f}%\".format(accuracy_cnn_train))\n",
        "\n",
        "#find the accuracy for each model for the test set\n",
        "accuracy_cnn_test= accuracy(cnn1, X_test.permute(0, 3, 1, 2), y_test)\n",
        "\n",
        "#display the results\n",
        "print(\"TESTING ACCURACY: : {:.2f}%\".format(accuracy_cnn_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYvDBqWye1Yo",
        "outputId": "d21b8f73-bcca-4833-dd66-dec2ec877c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolutional Neural Network 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-13683a255f41>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  o2 = func.softmax(self.fc2(o1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING ACCURACY: 16.67%\n",
            "TESTING ACCURACY: : 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 Evaluation\n",
        "\n",
        "In both the training and testing set the model managed to acheive a 16.67% accuracy rating. This is bad but not entirely to be unexpected. Given that we are processing colored images of size 150x150, in order to capture enough detail to be really accurate the model would probably have be much more complex than the one implemented above. Additionally one barrier which might have prevented the model from acheiving a higher accuracy would be a vanishing gradient which might explain why loss between minibatches does not seem to change too much. Something else which I think is happening in this scenario is that the model is getting stuck in a local minima when preforming gradient descent. I think this might explain why the loss suddenly increases in some instances, even after decreasing steadily for multiple minibatches in a row. It also makes some sense when looking at the accuracy. 16.67% is pretty much exactly the same odds as one would get if they were to guess randomly. It could be the machine found guessing randomly to be the most immediate way to icrease accuracy. Finally I would just like to note that I did try altering many of the hyperparameters in order to increase the accuracy but the effects were ultimately negligable. All that being said the model was quite quick to train which I suppose was the one upside. I do suspect this issue may not be the case when running resnet50.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yi-z5alP4tzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "aBsta1KHn2L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# intialize the model\n",
        "resnet50 = models.resnet50()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # convert tensor to PIL(python image library) Image\n",
        "    transforms.Resize((224, 224)), # resize to 224x224 which is what the model expects\n",
        "    transforms.ToTensor(), # convert back to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalize tensor\n",
        "])\n",
        "\n",
        "# Apply transformations to each sample in the tensor dataset\n",
        "transformed_X_train = torch.stack([transform(img) for img in X_train.permute(0, 3, 1, 2)])\n",
        "transformed_y_train = torch.argmax(y_train, dim=1)\n",
        "\n",
        "print(transformed_X_train.shape)\n",
        "\n",
        "#train the model\n",
        "train(resnet50, transformed_X_train, transformed_y_train, 3, 48, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaco1OPjxm7R",
        "outputId": "83a6c3a9-4f0a-409c-d872-b344d9d9a3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([480, 3, 224, 224])\n",
            "EPOCH: 1\n",
            "\tminibatch 0   LOSS: 7.715726   [   48/  480]\n",
            "\tminibatch 1   LOSS: 4.320488   [   96/  480]\n",
            "\tminibatch 2   LOSS: 2.073392   [  144/  480]\n",
            "\tminibatch 3   LOSS: 3.279970   [  192/  480]\n",
            "\tminibatch 4   LOSS: 2.207946   [  240/  480]\n",
            "\tminibatch 5   LOSS: 2.224346   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.702747   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.748207   [  384/  480]\n",
            "\tminibatch 8   LOSS: 2.127285   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.621047   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 2.902116\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.589665   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.413880   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.166868   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.092967   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.512559   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.493371   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.819302   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.302781   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.963434   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.197525   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.455235\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.273021   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.003484   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.098621   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.181593   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.269571   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.113959   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.024748   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.163648   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.977390   [  432/  480]\n",
            "\tminibatch 9   LOSS: 0.999010   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.210504\n",
            "\n",
            "TIME: 1194.49 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the transformation to the testing set\n",
        "transformed_X_test = torch.stack([transform(img) for img in X_test.permute(0, 3, 1, 2)])\n",
        "transformed_y_test = torch.argmax(y_test, dim=1)"
      ],
      "metadata": {
        "id": "zBHafUcfeZ1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training accuracy will not be calculated to avoid crashing due to memory limitations\n",
        "\n",
        "# #find the accuracy for each model for the train set\n",
        "# accuracy_resnet50_train = accuracy(resnet50, transformed_X_train, transformed_y_train)\n",
        "\n",
        "# #display the results\n",
        "# print(\"TRAINING ACCURACY: {:.2f}%\".format(accuracy_resnet50_train))\n",
        "\n",
        "#generate predictions and convert them to labels\n",
        "resnet50_y_preds = resnet50(transformed_X_test) #probabilites"
      ],
      "metadata": {
        "id": "qsxNAB1afTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, resnet50_y_preds = torch.max(resnet50_y_preds, 1)\n",
        "\n",
        "#compute the accuracy\n",
        "accuracy_resnet50_test = 100 * torch.eq(transformed_y_test, resnet50_y_preds).sum()/transformed_y_test.size(0)\n",
        "\n",
        "#display the results\n",
        "print(\"TRAINING ACCURACY: : {:.2f}%\".format(accuracy_resnet50_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF7sXa7kxzxZ",
        "outputId": "4a86f7e8-0965-465d-bf41-fad558eb312b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING ACCURACY: : 53.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50 Evaluation\n",
        "\n",
        "(NOTE: the training data accuracy has been ommitted during this step to avoid crashing, I tried. As a result this evaluation will only be comparing the testing data results.)\n",
        "\n",
        "The resnet50 model, preformed significantly better than the prior basic CNN. I think one of the major reasons this is the case is because the structure of the residual neural network incorporates skip connections which can remedy the problem of the vanishing gradient. Additionally, ResNet-50 is simply a much more complex model, containing 48 convolutinal layer. This model was also incredibly intensive in terms of memory. During the course of training, I just barely managed to stay under the system cap of 12.7 GB. At its peak the model was using around 12 GB of memory total. Additionally the model took a considerable amount of time to train, roughly 20 min in total.\n"
      ],
      "metadata": {
        "id": "c07oikl48wE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "hrV8E1Een4zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# define a randomized augmentation on the image\n",
        "augment = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip()\n",
        "])\n",
        "\n",
        "# apply the augmentations to the dataset\n",
        "augmented_X_train = torch.stack([augment(img) for img in X_train.permute(0, 3, 1, 2)])"
      ],
      "metadata": {
        "id": "6vglqCq1ZJR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize another convolution neural network\n",
        "cnn2 = CNN()\n",
        "\n",
        "# train the neural network on the augmented data\n",
        "train(cnn2, augmented_X_train, y_train, 3, 48, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Cv62G7dYgf",
        "outputId": "4b630854-a6f9-4a68-f93a-d04679aa32f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-13683a255f41>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  o2 = func.softmax(self.fc2(o1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tminibatch 0   LOSS: 1.825236   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.856092   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.960258   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.918592   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.814425   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.876925   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.876925   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.856092   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.856092   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.856092   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.869673\n",
            "EPOCH: 2\n",
            "\tminibatch 0   LOSS: 1.876925   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.918592   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.835258   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.876925   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.814425   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.876925   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.835258   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.876925   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.918592   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.939425   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.876925\n",
            "EPOCH: 3\n",
            "\tminibatch 0   LOSS: 1.876925   [   48/  480]\n",
            "\tminibatch 1   LOSS: 1.918592   [   96/  480]\n",
            "\tminibatch 2   LOSS: 1.876925   [  144/  480]\n",
            "\tminibatch 3   LOSS: 1.835258   [  192/  480]\n",
            "\tminibatch 4   LOSS: 1.856092   [  240/  480]\n",
            "\tminibatch 5   LOSS: 1.856092   [  288/  480]\n",
            "\tminibatch 6   LOSS: 1.876925   [  336/  480]\n",
            "\tminibatch 7   LOSS: 1.897758   [  384/  480]\n",
            "\tminibatch 8   LOSS: 1.897758   [  432/  480]\n",
            "\tminibatch 9   LOSS: 1.876925   [  480/  480]\n",
            "\n",
            "\tAVG. LOSS: 1.876925\n",
            "\n",
            "TIME: 112.79 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Convolutional Neural Network 2\\n\")\n",
        "\n",
        "#find the accuracy for each model for the test set\n",
        "accuracy_cnn_test_2= accuracy(cnn2, X_test.permute(0, 3, 1, 2), y_test)\n",
        "\n",
        "#display the results\n",
        "print(\"TESTING ACCURACY: : {:.2f}%\".format(accuracy_cnn_test_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpU6JCqXFGhW",
        "outputId": "e823d2e2-63cc-493a-dc02-a1a55130a256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolutional Neural Network 2\n",
            "\n",
            "TESTING ACCURACY: : 16.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-13683a255f41>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  o2 = func.softmax(self.fc2(o1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find the accuracy for each model for the train set\n",
        "accuracy_cnn_train_2 = accuracy(cnn2, X_train.permute(0, 3, 1, 2), y_train)\n",
        "\n",
        "#display the results\n",
        "print(\"TRAINING ACCURACY: {:.2f}%\".format(accuracy_cnn_train_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJlSaLYsQLAE",
        "outputId": "8e464185-1f56-452b-aced-d71f10fe4589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-13683a255f41>:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  o2 = func.softmax(self.fc2(o1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING ACCURACY: 16.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "The testing and training accuracy more or less have not changed from the first implmentation. This is probably for the same reason as was stated in that first evaluation. Were the model to be say, ResNet-50, the augmenting of this data would probably have helped to avoid overfitting the data however. Depending on the test set, this could have decreased the accuracy of the model or increased the accuracy, it is not really possible to say. Once again this model ran very quick, much quicker than the ResNet-50 model.\n"
      ],
      "metadata": {
        "id": "BAbnUF53Qdcj"
      }
    }
  ]
}